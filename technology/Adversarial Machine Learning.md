#Method 


This method attempts to fool machine learning models by supplying misleading inputs with the main goal of inducing a malfunction in a machine learning model. For this reason, it is deliberately used to train machine learning models in order to make them more robust and secure. Some known employed strategies are: evasion (samples are modified to evade detection), poisoning (adversarial contamination of training data), and model stealing (a black box machine learning system is used to reconstruct the model or extract the data it was trained on.)